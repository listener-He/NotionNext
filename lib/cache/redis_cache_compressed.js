import BLOG from '@/blog.config'
import { siteConfig } from '@/lib/config'
import Redis from 'ioredis'

// zstd-codec åŠ¨æ€å¯¼å…¥å’Œåˆå§‹åŒ–
let zstdCodec = null
let compressionAvailable = false

// åˆå§‹åŒ– zstd-codec
async function initZstdCodec() {
  try {
    const { ZstdCodec } = await import('zstd-codec')
    return new Promise((resolve) => {
      ZstdCodec.run(zstd => {
        zstdCodec = zstd
        compressionAvailable = true
        resolve(zstd)
      })
    })
  } catch (error) {
    console.warn('âš ï¸ ZSTD codec not available:', error.message)
    compressionAvailable = false
    return null
  }
}

// åˆå§‹åŒ– zstd-codec
initZstdCodec()

// åˆ†å—å¤§å°é…ç½®
const CHUNK_SIZE = 500 * 1024 // 500KB åˆ†å—å¤§å°
const MAX_TOTAL_SIZE = 50 * 1024 * 1024 // 50MB æ€»å¤§å°é™åˆ¶

// å‹ç¼©å‡½æ•° - æ”¯æŒåˆ†å—å‹ç¼©ï¼Œä¸å¯¹å¤–æŠ›å‡ºå¼‚å¸¸
function compressData(data, level = 3) {
  try {
    if (!compressionAvailable || !zstdCodec) {
      console.warn('âš ï¸ ZSTD codec not available, returning original data')
      return data
    }
    
    // éªŒè¯è¾“å…¥æ•°æ®
    if (!data || data.length === 0) {
      return new Uint8Array(0)
    }
    
    // æ£€æŸ¥æ•°æ®å¤§å°é™åˆ¶
    if (data.length > MAX_TOTAL_SIZE) {
      console.warn(`âš ï¸ Data too large for compression: ${(data.length/1024/1024).toFixed(1)}MB > ${MAX_TOTAL_SIZE/1024/1024}MB limit`)
      return data // è¿”å›åŸå§‹æ•°æ®è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
    }
    
    const simple = new zstdCodec.Simple()
    
    // å¦‚æœæ•°æ®å°äºåˆ†å—å¤§å°ï¼Œç›´æ¥å‹ç¼©
    if (data.length <= CHUNK_SIZE) {
      return simple.compress(data, level)
    }
    
    // åˆ†å—å‹ç¼©
    const chunks = []
    const chunkCount = Math.ceil(data.length / CHUNK_SIZE)
    
    for (let i = 0; i < chunkCount; i++) {
      const start = i * CHUNK_SIZE
      const end = Math.min(start + CHUNK_SIZE, data.length)
      const chunk = data.slice(start, end)
      
      if (chunk.length === 0) {
        throw new Error(`Empty chunk ${i} detected during compression`)
      }
      
      const compressedChunk = simple.compress(chunk, level)
      if (!compressedChunk || compressedChunk.length === 0) {
        throw new Error(`Failed to compress chunk ${i} or result is empty`)
      }
      
      chunks.push(compressedChunk)
    }
    
    // åˆ›å»ºåˆ†å—å‹ç¼©æ ‡è¯†å’Œå…ƒæ•°æ®
    const metadata = {
      isChunked: true,
      chunkCount: chunks.length,
      originalSize: data.length,
      chunkSizes: chunks.map(chunk => chunk.length)
    }
    
    // éªŒè¯å…ƒæ•°æ®å®Œæ•´æ€§
    if (metadata.chunkCount !== chunks.length || metadata.chunkSizes.length !== chunks.length) {
      throw new Error('Metadata inconsistency: chunk count mismatch')
    }
    
    const totalCompressedSize = metadata.chunkSizes.reduce((sum, size) => sum + size, 0)
    const actualCompressedSize = chunks.reduce((sum, chunk) => sum + chunk.length, 0)
    if (totalCompressedSize !== actualCompressedSize) {
      throw new Error('Metadata inconsistency: compressed size mismatch')
    }
    
    // å°†å…ƒæ•°æ®åºåˆ—åŒ–å¹¶å‹ç¼©
    const metadataBuffer = Buffer.from(JSON.stringify(metadata), 'utf8')
    const compressedMetadata = simple.compress(new Uint8Array(metadataBuffer), level)
    
    // ç»„åˆï¼šå…ƒæ•°æ®é•¿åº¦(4å­—èŠ‚) + å‹ç¼©å…ƒæ•°æ® + æ‰€æœ‰å‹ç¼©å—
    const metadataLengthBuffer = new ArrayBuffer(4)
    const metadataLengthView = new DataView(metadataLengthBuffer)
    metadataLengthView.setUint32(0, compressedMetadata.length, true)
    
    const totalLength = 4 + compressedMetadata.length + chunks.reduce((sum, chunk) => sum + chunk.length, 0)
    const result = new Uint8Array(totalLength)
    
    let offset = 0
    result.set(new Uint8Array(metadataLengthBuffer), offset)
    offset += 4
    result.set(compressedMetadata, offset)
    offset += compressedMetadata.length
    
    for (const chunk of chunks) {
      result.set(chunk, offset)
      offset += chunk.length
    }
    
    return result
  } catch (error) {
    console.error('âš ï¸ ZSTD compression error, returning original data:', error.message)
    return data // è¿”å›åŸå§‹æ•°æ®è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
  }
}

// è§£å‹å‡½æ•° - å…¼å®¹åˆ†å—å‹ç¼©ï¼Œä¸å¯¹å¤–æŠ›å‡ºå¼‚å¸¸
function decompressData(compressedData) {
  try {
    if (!compressionAvailable || !zstdCodec) {
      console.warn('âš ï¸ ZSTD codec not available, returning original data')
      return compressedData
    }
    
    if (!compressedData || compressedData.length === 0) {
      return new Uint8Array(0)
    }
    
    const simple = new zstdCodec.Simple()
    
    // æ£€æŸ¥æ˜¯å¦ä¸ºåˆ†å—å‹ç¼©æ•°æ®ï¼ˆè‡³å°‘éœ€è¦4å­—èŠ‚çš„å…ƒæ•°æ®é•¿åº¦ï¼‰
    if (compressedData.length < 4) {
      // ç›´æ¥è§£å‹ç¼©
      return simple.decompress(compressedData)
    }
    
    try {
      // å°è¯•è¯»å–å…ƒæ•°æ®é•¿åº¦
      const metadataLengthView = new DataView(compressedData.buffer, compressedData.byteOffset, 4)
      const metadataLength = metadataLengthView.getUint32(0, true)
      
      // éªŒè¯å…ƒæ•°æ®é•¿åº¦æ˜¯å¦åˆç†
      if (metadataLength > 0 && metadataLength < compressedData.length - 4) {
        // æå–å¹¶è§£å‹å…ƒæ•°æ®
        const compressedMetadata = compressedData.slice(4, 4 + metadataLength)
        const metadataBuffer = simple.decompress(compressedMetadata)
        const metadataString = Buffer.from(metadataBuffer).toString('utf8')
        const metadata = JSON.parse(metadataString)
        
        // æ£€æŸ¥æ˜¯å¦ä¸ºåˆ†å—å‹ç¼©æ•°æ®
        if (metadata.isChunked && metadata.chunkCount && metadata.chunkSizes && 
            metadata.originalSize && metadata.chunkCount > 0 && 
            Array.isArray(metadata.chunkSizes) && metadata.chunkSizes.length === metadata.chunkCount) {
          // åˆ†å—è§£å‹ç¼©
          const chunks = []
          let offset = 4 + metadataLength
          
          for (let i = 0; i < metadata.chunkCount; i++) {
            const chunkSize = metadata.chunkSizes[i]
            if (chunkSize <= 0 || offset + chunkSize > compressedData.length) {
              throw new Error(`Invalid chunk ${i}: size=${chunkSize}, offset=${offset}, total=${compressedData.length}`)
            }
            
            const compressedChunk = compressedData.slice(offset, offset + chunkSize)
            if (compressedChunk.length !== chunkSize) {
              throw new Error(`Chunk ${i} size mismatch: expected ${chunkSize}, got ${compressedChunk.length}`)
            }
            
            const decompressedChunk = simple.decompress(compressedChunk)
            if (!decompressedChunk || decompressedChunk.length === 0) {
              throw new Error(`Failed to decompress chunk ${i} or chunk is empty`)
            }
            
            chunks.push(decompressedChunk)
            offset += chunkSize
          }
          
          // åˆå¹¶æ‰€æœ‰è§£å‹ç¼©çš„å—
          const totalLength = chunks.reduce((sum, chunk) => sum + chunk.length, 0)
          if (totalLength !== metadata.originalSize) {
            throw new Error(`Decompressed size mismatch: expected ${metadata.originalSize}, got ${totalLength}`)
          }
          
          const result = new Uint8Array(metadata.originalSize)
          let resultOffset = 0
          for (const chunk of chunks) {
            if (resultOffset + chunk.length > metadata.originalSize) {
              throw new Error('Chunk data exceeds expected original size')
            }
            result.set(chunk, resultOffset)
            resultOffset += chunk.length
          }
          
          return result
        }
      }
    } catch (metadataError) {
      // å¦‚æœå…ƒæ•°æ®è§£æå¤±è´¥ï¼Œå°è¯•ç›´æ¥è§£å‹ç¼©
      console.log('ğŸ“¦ Not chunked data or metadata parse failed, trying direct decompression')
    }
    
    // ç›´æ¥è§£å‹ç¼©ï¼ˆéåˆ†å—æ•°æ®æˆ–å…ƒæ•°æ®è§£æå¤±è´¥ï¼‰
    return simple.decompress(compressedData)
  } catch (error) {
    console.error('âš ï¸ ZSTD decompression error, returning original data:', error.message)
    return compressedData // è¿”å›åŸå§‹æ•°æ®è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
  }
}

// è¿æ¥æ± é…ç½®
// è§£æ Redis URL æˆ–ä½¿ç”¨é»˜è®¤é…ç½®
let redisConfig = {}
if (BLOG.REDIS_URL) {
  try {
    const url = new URL(BLOG.REDIS_URL)
    redisConfig = {
      host: url.hostname,
      port: parseInt(url.port) || 6379,
      username: url.username || undefined,
      password: url.password || undefined,
      db: url.pathname && url.pathname.length > 1 ? parseInt(url.pathname.slice(1)) : 0
    }
    // console.log('âœ… Redis URL parsed successfully:', `${url.hostname}:${url.port}`)
  } catch (error) {
    console.warn('âš ï¸ Invalid Redis URL, using default config:', error.message)
    redisConfig = { host: '127.0.0.1', port: 6379 }
  }
} else {
  redisConfig = { host: '127.0.0.1', port: 6379 }
}

// Redisè¿æ¥æ± é…ç½®
const REDIS_POOL_CONFIG = {
  maxConnections: 30,        // æœ€å¤§è¿æ¥æ•°
  minConnections: 2,         // æœ€å°è¿æ¥æ•°
  acquireTimeoutMillis: 5000, // è·å–è¿æ¥è¶…æ—¶æ—¶é—´
  idleTimeoutMillis: 60000,   // è¿æ¥ç©ºé—²è¶…æ—¶æ—¶é—´
  maxWaitingClients: 50       // æœ€å¤§ç­‰å¾…å®¢æˆ·ç«¯æ•°
}

// è¿æ¥ç­‰å¾…é˜Ÿåˆ—
const connectionQueue = []
let activeConnections = 0

const redisClient = new Redis({
  ...redisConfig,
  connectTimeout: 3000, // è¶…æ—¶æ—¶é—´3ç§’
  maxRetriesPerRequest: 2,
  reconnectOnError: (err) => {
    console.log('Redis reconnecting due to error:', err.message)
    return true
  },
  retryStrategy(times) {
    const delay = Math.min(times * 100, 3000)
    console.log(`Redis retry attempt ${times}, delay: ${delay}ms`)
    return delay
  },
  connectionName: 'blog-redis-client',
  lazyConnect: true,
  keepAlive: 30000,
  family: 4, // å¼ºåˆ¶ä½¿ç”¨IPv4
  // è¿æ¥æ± ç›¸å…³é…ç½®
  maxRetriesPerRequest: REDIS_POOL_CONFIG.maxConnections,
  enableAutoPipelining: true,
  maxLoadingTimeout: 5000
})

// è¿æ¥æ± ç®¡ç†å‡½æ•°
async function acquireConnection() {
  return new Promise((resolve, reject) => {
    const timeout = setTimeout(() => {
      const index = connectionQueue.findIndex(item => item.resolve === resolve)
      if (index !== -1) {
        connectionQueue.splice(index, 1)
      }
      reject(new Error('Connection acquire timeout'))
    }, REDIS_POOL_CONFIG.acquireTimeoutMillis)

    if (activeConnections < REDIS_POOL_CONFIG.maxConnections) {
      activeConnections++
      clearTimeout(timeout)
      resolve(redisClient)
    } else if (connectionQueue.length < REDIS_POOL_CONFIG.maxWaitingClients) {
      connectionQueue.push({ resolve, reject, timeout })
    } else {
      clearTimeout(timeout)
      reject(new Error('Connection pool exhausted'))
    }
  })
}

function releaseConnection() {
  activeConnections = Math.max(0, activeConnections - 1)
  
  if (connectionQueue.length > 0) {
    const { resolve, timeout } = connectionQueue.shift()
    clearTimeout(timeout)
    activeConnections++
    resolve(redisClient)
  }
}

// Redisè¿æ¥äº‹ä»¶ç›‘å¬
redisClient.on('connect', () => {
  // console.log('âœ… Redis connected successfully')
})

redisClient.on('error', (err) => {
  console.error('âŒ Redis connection error:', err.message)
})

redisClient.on('close', () => {
  // console.log('ğŸ”Œ Redis connection closed')
})

redisClient.on('reconnecting', () => {
  console.log('ğŸ”„ Redis reconnecting...')
})

const cacheTime = Math.trunc(
  siteConfig('NEXT_REVALIDATE_SECOND', BLOG.NEXT_REVALIDATE_SECOND) * 1.5
)

// å‹ç¼©é˜ˆå€¼ï¼šè¶…è¿‡æ­¤å¤§å°çš„æ•°æ®æ‰è¿›è¡Œå‹ç¼©ï¼ˆå­—èŠ‚ï¼‰
const COMPRESSION_THRESHOLD = 1 * 1024 // 1KB

// åŠ¨æ€å‹ç¼©çº§åˆ«é…ç½®ï¼šæ ¹æ®æ•°æ®å¤§å°æ™ºèƒ½è°ƒæ•´å‹ç¼©çº§åˆ«ï¼ˆ6ä¸ªçº§åˆ«ï¼‰
const COMPRESSION_LEVELS = {
  level1: 3,    // 1KB-10KB: æœ€ä½çº§åˆ«ï¼Œé€Ÿåº¦ä¼˜å…ˆ
  level2: 6,    // 10KB-50KB: ä½çº§åˆ«ï¼Œå¹³è¡¡æ€§èƒ½
  level3: 9,    // 50KB-100KB: ä¸­ç­‰çº§åˆ«
  level4: 12,   // 100KB-200KB: ä¸­é«˜çº§åˆ«
  level5: 15,   // 200KB-499KB: é«˜çº§åˆ«
  level6: 22    // >=500KB: æœ€é«˜çº§åˆ«ï¼Œå‹ç¼©ç‡ä¼˜å…ˆ
}
const SIZE_THRESHOLDS = {
  level1: 10 * 1024,   // 10KB
  level2: 50 * 1024,   // 50KB
  level3: 100 * 1024,  // 100KB
  level4: 200 * 1024,  // 200KB
  level5: 499 * 1024   // 499KB
}

const MIN_COMPRESSION_RATIO = 0.15 // æœ€å°å‹ç¼©æ•ˆæœé˜ˆå€¼ (15%èŠ‚çœç©ºé—´æ‰ä½¿ç”¨å‹ç¼©)

/**
 * æ ¹æ®æ•°æ®å¤§å°åŠ¨æ€é€‰æ‹©å‹ç¼©çº§åˆ«
 * @param {number} dataSize - æ•°æ®å¤§å°ï¼ˆå­—èŠ‚ï¼‰
 * @returns {number} å‹ç¼©çº§åˆ«
 */
function getDynamicCompressionLevel(dataSize) {
  if (dataSize <= SIZE_THRESHOLDS.level1) return COMPRESSION_LEVELS.level1
  if (dataSize <= SIZE_THRESHOLDS.level2) return COMPRESSION_LEVELS.level2
  if (dataSize <= SIZE_THRESHOLDS.level3) return COMPRESSION_LEVELS.level3
  if (dataSize <= SIZE_THRESHOLDS.level4) return COMPRESSION_LEVELS.level4
  if (dataSize <= SIZE_THRESHOLDS.level5) return COMPRESSION_LEVELS.level5
  return COMPRESSION_LEVELS.level6
}

/**
 * ä»ç¼“å­˜ä¸­è·å–æ•°æ® - ä½¿ç”¨è¿æ¥æ± ï¼Œä¸å¯¹å¤–æŠ›å‡ºå¼‚å¸¸
 */
export async function getCache(key) {
  let connection = null
  try {
    // éªŒè¯è¾“å…¥å‚æ•°
    if (!key || typeof key !== 'string') {
      console.warn('âš ï¸ Invalid cache key provided')
      return null
    }

    // è·å–è¿æ¥
    connection = await acquireConnection()
    const value = await connection.get(key)
    
    if (!value) {
      return null
    }

    // æ£€æŸ¥æ˜¯å¦ä¸ºå‹ç¼©æ•°æ®
    if (value.startsWith('ZSTD:')) {
      if (!compressionAvailable) {
        console.warn('âš ï¸ Found compressed data but ZSTD codec not available')
        return null
      }
      
      try {
        const compressedData = Buffer.from(value.slice(5), 'base64')
        const decompressed = decompressData(new Uint8Array(compressedData))
        const jsonString = Buffer.from(decompressed).toString('utf8')
        return JSON.parse(jsonString)
      } catch (decompressionError) {
        console.error('âš ï¸ Cache decompression failed:', decompressionError.message)
        return null
      }
    }

    try {
      return JSON.parse(value)
    } catch (parseError) {
      console.error('âš ï¸ Cache JSON parse failed:', parseError.message)
      return null
    }
  } catch (error) {
    console.error('âš ï¸ Cache get error:', error.message)
    return null
  } finally {
    // é‡Šæ”¾è¿æ¥
    if (connection) {
      releaseConnection()
    }
  }
}

/**
 * è®¾ç½®ç¼“å­˜æ•°æ® - ä½¿ç”¨è¿æ¥æ± ï¼Œä¸å¯¹å¤–æŠ›å‡ºå¼‚å¸¸
 */
export async function setCache(key, data, customCacheTime) {
  let connection = null
  try {
    // éªŒè¯è¾“å…¥å‚æ•°
    if (!key || typeof key !== 'string') {
      console.warn('âš ï¸ Invalid cache key provided')
      return null
    }
    
    if (data === undefined || data === null) {
      console.warn('âš ï¸ Cannot cache undefined or null data')
      return null
    }
    
    let jsonString
    try {
      jsonString = JSON.stringify(data)
    } catch (serializeError) {
      console.error('âš ï¸ Failed to serialize data to JSON:', serializeError.message)
      return null
    }
    
    if (!jsonString || jsonString === 'undefined') {
      console.warn('âš ï¸ Invalid JSON serialization result')
      return null
    }
    
    const dataBuffer = Buffer.from(jsonString, 'utf8')
    const dataSize = dataBuffer.length
    let finalValue = jsonString
    let compressionStats = null

    // æ™ºèƒ½å‹ç¼©ç­–ç•¥ï¼šä»…å¯¹å¤§äºé˜ˆå€¼çš„æ•°æ®è¿›è¡Œå‹ç¼©
    if (compressionAvailable && dataSize > COMPRESSION_THRESHOLD) {
      const compressionLevel = getDynamicCompressionLevel(dataSize)
      const startTime = Date.now()
      
      try {
        const uint8Array = new Uint8Array(dataBuffer)
        const compressed = compressData(uint8Array, compressionLevel)
        
        // æ£€æŸ¥å‹ç¼©æ˜¯å¦æˆåŠŸï¼ˆcompressDataç°åœ¨ä¸ä¼šæŠ›å‡ºå¼‚å¸¸ï¼‰
        if (compressed && compressed !== uint8Array) {
          const compressedSize = compressed.length
          const compressionTime = Date.now() - startTime
          const compressionRatio = (dataSize - compressedSize) / dataSize
          
          compressionStats = {
            originalSize: dataSize,
            compressedSize,
            compressionRatio,
            compressionTime,
            level: compressionLevel
          }
          
          // ä»…åœ¨å‹ç¼©æ•ˆæœæ˜¾è‘—æ—¶ä½¿ç”¨å‹ç¼©ï¼ˆèŠ‚çœç©ºé—´â‰¥15%ï¼‰
          if (compressionRatio >= MIN_COMPRESSION_RATIO) {
            finalValue = 'ZSTD:' + Buffer.from(compressed).toString('base64')
            console.log(`ğŸ—œï¸ Cache compressed: ${(dataSize/1024).toFixed(1)}KB â†’ ${(compressedSize/1024).toFixed(1)}KB (${(compressionRatio*100).toFixed(1)}% saved, level ${compressionLevel}, ${compressionTime}ms)`)
          } else {
            console.log(`ğŸ“¦ Cache not compressed: ratio ${(compressionRatio*100).toFixed(1)}% < ${(MIN_COMPRESSION_RATIO*100)}% threshold`)
          }
        } else {
          console.log('ğŸ“¦ Compression returned original data, storing uncompressed')
        }
      } catch (compressionError) {
        console.warn('âš ï¸ Compression failed, storing uncompressed:', compressionError.message)
      }
    } else if (!compressionAvailable && dataSize > COMPRESSION_THRESHOLD) {
      console.log('ğŸ“¦ Large data detected but ZSTD codec not available, storing uncompressed')
    }

    // è·å–è¿æ¥å¹¶å­˜å‚¨æ•°æ®
    connection = await acquireConnection()
    const ttl = customCacheTime || cacheTime
    await connection.setex(key, ttl, finalValue)
    
    return compressionStats
  } catch (error) {
    console.error('âš ï¸ Cache set error:', error.message)
    return null
  } finally {
    // é‡Šæ”¾è¿æ¥
    if (connection) {
      releaseConnection()
    }
  }
}

/**
 * åˆ é™¤ç¼“å­˜ - ä½¿ç”¨è¿æ¥æ± ï¼Œä¸å¯¹å¤–æŠ›å‡ºå¼‚å¸¸
 */
export async function delCache(key) {
  let connection = null
  try {
    if (!key || typeof key !== 'string') {
      console.warn('âš ï¸ Invalid cache key provided')
      return 0
    }

    connection = await acquireConnection()
    return await connection.del(key)
  } catch (error) {
    console.error('âš ï¸ Cache delete error:', error.message)
    return 0
  } finally {
    if (connection) {
      releaseConnection()
    }
  }
}

/**
 * è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯ - ä½¿ç”¨è¿æ¥æ± ï¼Œä¸å¯¹å¤–æŠ›å‡ºå¼‚å¸¸
 */
export async function getCacheStats(keyPattern = '*') {
  let connection = null
  try {
    if (!keyPattern || typeof keyPattern !== 'string') {
      keyPattern = '*'
    }

    connection = await acquireConnection()
    const keys = await connection.keys(keyPattern)
    
    if (keys.length === 0) {
      return { 
        totalKeys: 0, 
        totalSize: 0, 
        compressedKeys: 0, 
        uncompressedKeys: 0,
        connectionPoolStats: {
          activeConnections,
          queueLength: connectionQueue.length,
          maxConnections: REDIS_POOL_CONFIG.maxConnections
        }
      }
    }

    // é‡‡æ ·å‰100ä¸ªé”®è¿›è¡Œç»Ÿè®¡
    const sampleKeys = keys.slice(0, 100)
    const values = await connection.mget(sampleKeys)
    
    let totalSize = 0
    let compressedKeys = 0
    let uncompressedKeys = 0
    
    values.forEach(value => {
      if (value) {
        totalSize += Buffer.byteLength(value, 'utf8')
        if (value.startsWith('ZSTD:')) {
          compressedKeys++
        } else {
          uncompressedKeys++
        }
      }
    })
    
    return {
      totalKeys: keys.length,
      sampledKeys: sampleKeys.length,
      totalSize: Math.round(totalSize / 1024), // KB
      compressedKeys,
      uncompressedKeys,
      compressionRatio: compressedKeys > 0 ? compressedKeys / (compressedKeys + uncompressedKeys) : 0,
      connectionPoolStats: {
        activeConnections,
        queueLength: connectionQueue.length,
        maxConnections: REDIS_POOL_CONFIG.maxConnections
      }
    }
  } catch (error) {
    console.error('âš ï¸ Cache stats error:', error.message)
    return { 
      error: error.message,
      connectionPoolStats: {
        activeConnections,
        queueLength: connectionQueue.length,
        maxConnections: REDIS_POOL_CONFIG.maxConnections
      }
    }
  } finally {
    if (connection) {
      releaseConnection()
    }
  }
}

export default { getCache, setCache, delCache, getCacheStats }